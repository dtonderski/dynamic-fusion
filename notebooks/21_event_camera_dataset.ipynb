{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davton/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from dynamic_fusion.network_trainer.configuration import TrainerConfiguration\n",
    "from dynamic_fusion.network_trainer.network_loader import NetworkLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_fusion.scripts.test_e2vid_data import get_events_from_txt\n",
    "import pandas as pd\n",
    "directory = Path(\"../data/raw/event_camera_dataset/dynamic_6dof\")\n",
    "\n",
    "\n",
    "events, _, _ = get_events_from_txt(directory / \"events.txt\", first_row_is_image_shape=False, max_t = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_metadata = pd.read_csv(directory / \"images.txt\", delimiter=\" \", header=None, names = [\"timestamp\", \"path\"], dtype={\"timestamp\": np.float64, \"path\": str})\n",
    "\n",
    "events['frame_bin'] = pd.cut(events.timestamp, image_metadata.timestamp, labels=False, right = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_fusion.data_generator.configuration import EventDiscretizerConfiguration\n",
    "from dynamic_fusion.data_generator.event_discretizer import EventDiscretizer\n",
    "\n",
    "THRESHOLD = 1\n",
    "config = EventDiscretizerConfiguration(number_of_temporal_bins=1, number_of_temporal_sub_bins_per_bin=2)\n",
    "discretizer = EventDiscretizer(config, max_timestamp=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_fusion.utils.discretized_events import DiscretizedEvents\n",
    "\n",
    "\n",
    "discretized_frames = []\n",
    "\n",
    "for frame_bin, events_in_frame in events.groupby(\"frame_bin\"):\n",
    "    timestamp_range = (image_metadata.timestamp[frame_bin], image_metadata.timestamp[frame_bin + 1])\n",
    "\n",
    "    assert np.all((events_in_frame.timestamp < timestamp_range[1]) & (events_in_frame.timestamp >= timestamp_range[0]))\n",
    "    events_in_frame.timestamp -= timestamp_range[0]\n",
    "    events_in_frame.timestamp /= timestamp_range[1] - timestamp_range[0]\n",
    "    discretized_frame = discretizer._discretize_events(events_in_frame, THRESHOLD, (180, 240))\n",
    "    discretized_frames.append(discretized_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared=SharedConfiguration(sequence_length=16, resume=True, use_events=True, use_mean=True, use_std=True, use_count=True, implicit=True, spatial_unfolding=True, temporal_unfolding=True, temporal_interpolation=True, spatial_upscaling=True, predict_uncertainty=True, use_aps_for_all_frames=False, use_initial_aps_frame=False, min_allowed_max_of_mean_polarities_over_times=0.05) data_handler=DataHandlerConfiguration(augmentation=AugmentationConfiguration(network_image_size=(56, 56)), dataset=DatasetConfiguration(dataset_directory=PosixPath('/mnt/train/2subbins'), threshold=1.35, augmentation_tries=2, video_tries=5, max_upscaling=4.0), test_dataset_directory=PosixPath('/mnt/test/2subbins'), test_scale_range=(1, 4), batch_size=2, num_workers=1) network_loader=NetworkLoaderConfiguration(encoding=EncodingNetworkConfiguration(input_size=2, hidden_size=24, output_size=16, kernel_size=3), encoding_checkpoint_path=None, decoding=DecodingNetworkConfiguration(hidden_size=128, hidden_layers=4), decoding_checkpoint_path=None) network_fitter=NetworkFitterConfiguration(lr_reconstruction=0.0005, number_of_training_iterations=500000, reconstruction_loss_name='uncertainty_with_LPIPS', skip_first_timesteps=4, network_saving_frequency=100, visualization_frequency=200, gradient_application_period=1, uncertainty_weight=1) training_monitor=TrainingMonitorConfiguration(run_directory=PosixPath('runs/0323-new_dataset/00_st-un_st-interp_st-up_uncertainty-lpips'), persistent_saving_frequency=10000, Ts_to_visualize=20, taus_to_visualize=5, Ts_to_evaluate=40, taus_to_evaluate=5, test_samples_to_visualize=[0, 1, 2, 3, 5], lpips_batch=20, evaluation_period=2000) seed=0\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"e2vid_exp\"\n",
    "MODEL = \"e2vid_exp_uncertainty\"\n",
    "\n",
    "if MODEL == \"e2vid_exp\":\n",
    "    CHECKPOINT_DIR = Path(\"../runs/0323-new-dataset/01_st-un_st-interp_st-up/subrun_00\")\n",
    "elif MODEL == \"e2vid_exp_uncertainty\":\n",
    "    CHECKPOINT_DIR = Path(\"../runs/0323-new-dataset/00_st-un_st-interp_st-up_uncertainty-lpips/subrun_00\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "CHECKPOINT_NAME = \"latest_checkpoint.pt\"\n",
    "\n",
    "\n",
    "config_path = CHECKPOINT_DIR / \"config.json\"\n",
    "with config_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "    json_config = f.read()\n",
    "# Parse the JSON string back into a Configuration instance\n",
    "config = TrainerConfiguration.parse_raw(json_config)\n",
    "# Load network\n",
    "config.network_loader.decoding_checkpoint_path = CHECKPOINT_DIR / CHECKPOINT_NAME\n",
    "config.network_loader.encoding_checkpoint_path = CHECKPOINT_DIR / CHECKPOINT_NAME\n",
    "encoder, decoder = NetworkLoader(config.network_loader, config.shared).run()\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 / 108\r"
     ]
    }
   ],
   "source": [
    "from dynamic_fusion.scripts.test_e2vid_data import run_reconstruction\n",
    "from dynamic_fusion.utils.discretized_events import DiscretizedEvents\n",
    "\n",
    "\n",
    "discretized_events = DiscretizedEvents.stack_temporally(discretized_frames)\n",
    "reconstruction = run_reconstruction(encoder, decoder, discretized_events, device, config.shared)\n",
    "\n",
    "mins = reconstruction.min(axis=(2,3), keepdims=True)[:,0]\n",
    "maxs = reconstruction.max(axis=(2,3), keepdims=True)[:,0]\n",
    "\n",
    "reconstruction_norm = reconstruction.copy()\n",
    "\n",
    "reconstruction_norm[:,0] = (reconstruction_norm[:,0] - mins) / (maxs - mins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 2, 180, 240)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = reconstruction.min(axis=(2,3), keepdims=True)[:,0]\n",
    "maxs = reconstruction.max(axis=(2,3), keepdims=True)[:,0]\n",
    "\n",
    "reconstruction_norm = reconstruction.copy()\n",
    "\n",
    "reconstruction_norm[:,0] = (reconstruction_norm[:,0] - mins) / (maxs - mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.color import rgb2gray\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from dynamic_fusion.utils.loss import LPIPS\n",
    "from tqdm import tqdm\n",
    "\n",
    "images = []\n",
    "for path in image_metadata.path:\n",
    "    image = cv2.imread(str(directory / path))\n",
    "    gray = rgb2gray(image)\n",
    "    images.append(gray)\n",
    "\n",
    "images_np = np.stack(images, axis=0)\n",
    "\n",
    "\n",
    "ssim_vals = [ssim(reconstruction_norm[i,0], images[i], data_range=1) for i in range(len(reconstruction_norm))]\n",
    "print(sum(ssim_vals) / len(ssim_vals))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(ssim_vals)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "lpips_vals = []\n",
    "lpips = LPIPS().to(device)\n",
    "\n",
    "for i in tqdm(range(len(reconstruction_norm))):\n",
    "    recon_tensor = torch.tensor(reconstruction_norm[i, 0:1][None]).to(device).float()\n",
    "    image_tensor = torch.tensor(images[i][None, None]).to(device).float()\n",
    "    lpips_vals.append(lpips(recon_tensor, image_tensor).item())\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(lpips_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_fusion.utils.network import to_numpy\n",
    "from dynamic_fusion.utils.plotting import discretized_events_to_cv2_frame, add_text_at_row\n",
    "from dynamic_fusion.utils.visualization import create_red_blue_cmap, img_to_colormap\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "FRAMES = 40\n",
    "output_dir = Path('../results/event_camera_dataset_test/')\n",
    "output_dir.mkdir(parents=True,exist_ok=True)\n",
    "SPEED = 0.5\n",
    "size = discretized_events.event_polarity_sum.shape\n",
    "out = cv2.VideoWriter(f\"{str(output_dir)}/{directory.name}.mp4\", cv2.VideoWriter.fourcc(*\"mp4v\"), int(len(discretized_events.event_polarity_sum)/events.timestamp.max()*SPEED), (size[-1]*3, size[-2]))\n",
    "colored_event_polarity_sums = img_to_colormap(to_numpy(discretized_events.event_polarity_sum.sum(dim=1)), create_red_blue_cmap(501))\n",
    "for I in range(FRAMES):\n",
    "    event_frame = discretized_events_to_cv2_frame(colored_event_polarity_sums[I], discretized_events.event_count[I])\n",
    "    recon_frame = cv2.cvtColor((reconstruction[I, 0]*255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "    gt_frame = cv2.cvtColor((images[I]*255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    add_text_at_row(recon_frame, f\"LPIPS={lpips_vals[I]:.2f}\", 0)\n",
    "    add_text_at_row(recon_frame, f\"SSIM={ssim_vals[I]:.2f}\", 1)\n",
    "\n",
    "    frame = np.concatenate([event_frame, recon_frame, gt_frame], axis=1)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
