{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davton/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (video.py, line 146)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3550\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[1], line 3\u001b[0m\n    from dynamic_fusion.utils.dataset import CocoTestDataset\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/dev/dynamic-fusion/dynamic_fusion/utils/dataset.py:15\u001b[0;36m\n\u001b[0;31m    from dynamic_fusion.utils.video import get_video\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/dev/dynamic-fusion/dynamic_fusion/utils/video.py:146\u001b[0;36m\u001b[0m\n\u001b[0;31m    /def _transforms_to_matrices(shifts: Float[np.ndarray, \"T 2\"], rotations: Float[np.ndarray, \"T 1\"], scales: Float[np.ndarray, \"T 2\"]) -> Float[np.ndarray, \"T 3 3\"]:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from dynamic_fusion.utils.dataset import CocoTestDataset\n",
    "\n",
    "dataset_path = Path('..', 'data', 'interim', 'coco', 'train', '2subbins')\n",
    "\n",
    "\n",
    "metrics_temporal_scale = []\n",
    "spatial_scale = 1\n",
    "temporal_scale = 1\n",
    "dataset = CocoTestDataset(dataset_path, (spatial_scale, spatial_scale), threshold=1.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from dynamic_fusion.utils.network import to_numpy\n",
    "from dynamic_fusion.utils.visualization import create_red_blue_cmap, img_to_colormap\n",
    "import cv2\n",
    "\n",
    "import einops\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from dynamic_fusion.utils.video import get_video\n",
    "\n",
    "downscaled = False\n",
    "FPS = 30\n",
    "N_aps_frames = 20*FPS\n",
    "\n",
    "for I in range(0, 9):\n",
    "    if not downscaled:\n",
    "        epss,_,_,counts,_,_,_,_,preprocessed_image, transforms = dataset[I]\n",
    "    else:\n",
    "        _,_,_,_,epss,_,_,counts,preprocessed_image, transforms = dataset[I]\n",
    "\n",
    "    epss = to_numpy(epss)\n",
    "    counts = to_numpy(counts)\n",
    "    Ts_normalized = np.linspace(0,1, N_aps_frames)\n",
    "    video = get_video(preprocessed_image, transforms, Ts_normalized, downscaled, not downscaled, torch.device('cuda'))\n",
    "    video = to_numpy(video)\n",
    "\n",
    "    # epss = einops.reduce(epss, \"(C 2) B X Y -> C B X Y\", 'sum')\n",
    "    # counts = einops.reduce(counts, \"(C 2) B X Y -> C B X Y\", 'sum')\n",
    "\n",
    "    size = tuple(epss[0].shape[-2:])\n",
    "    # In total, should be 20 seconds\n",
    "    out = cv2.VideoWriter(f\"dynamic_fusion/{downscaled=}_{I}.mp4\", cv2.VideoWriter.fourcc(*\"mp4v\"), len(epss) // 2, (size[1], size[0]*2))\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.4 if not downscaled else 0.2\n",
    "    font_color = (255, 255, 255)  # White color\n",
    "    line_type = 1\n",
    "    position = (10, 10)  # Position of the text (bottom left corner)\n",
    "    position_1 = (10, 30)\n",
    "\n",
    "    all_frames = []\n",
    "\n",
    "    for i, image in enumerate(video):\n",
    "        i_event_frame = (eps.shape[0] * i) // N_aps_frames\n",
    "        eps, count = epss[i_event_frame], counts[i_event_frame]\n",
    "\n",
    "        colored_event_polarity_sum = img_to_colormap(eps.sum(axis=0), create_red_blue_cmap(501))\n",
    "\n",
    "        evr_frame = count.sum(axis=0).mean()\n",
    "        evr = evr_frame / 20 * len(video)\n",
    "\n",
    "        image_processed = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        if downscaled:\n",
    "            image_processed = cv2.resize(image_processed, (colored_event_polarity_sum.shape[1], colored_event_polarity_sum.shape[0]))\n",
    "\n",
    "        frame_processed = (colored_event_polarity_sum * 255).astype(np.uint8)\n",
    "        frame_processed = np.concatenate((image_processed, frame_processed), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        cv2.putText(frame_processed, f\"Events per second per pixel={evr:.2f}\", position, font, font_scale, font_color, line_type)\n",
    "        cv2.putText(frame_processed, f\"Events per frame per pixel={evr_frame:.2f}\", position_1, font, font_scale, font_color, line_type)\n",
    "\n",
    "\n",
    "        out.write(frame_processed)\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video has been saved to dynamic_fusion/video.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory containing videos\n",
    "video_dir = 'dynamic_fusion'\n",
    "# Output video file path\n",
    "output_video_path = 'dynamic_fusion/video.mp4'\n",
    "\n",
    "# Read all video files\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "video_paths = [os.path.join(video_dir, f) for f in video_files]\n",
    "\n",
    "# Open video files\n",
    "caps = [cv2.VideoCapture(vp) for vp in video_paths]\n",
    "\n",
    "# Determine the width, height, and FPS from the first video (assuming all are the same)\n",
    "width, height = int(caps[0].get(cv2.CAP_PROP_FRAME_WIDTH)), int(caps[0].get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = caps[0].get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Find the length of the longest video\n",
    "max_length = max(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) for cap in caps)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID'\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width * len(caps), height))\n",
    "\n",
    "# Read frames from each video, stack horizontally, and write to the output video\n",
    "for i in range(max_length):\n",
    "    frames = []\n",
    "    for cap in caps:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            # If the video is shorter than the longest one, use the last frame\n",
    "            frame = np.zeros((height, width, 3), np.uint8)\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Stack frames horizontally\n",
    "    stacked_frame = np.hstack(frames)\n",
    "    \n",
    "    # Write the stacked frame to the output video\n",
    "    out.write(stacked_frame)\n",
    "\n",
    "# Release everything when done\n",
    "for cap in caps:\n",
    "    cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f'Output video has been saved to {output_video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
